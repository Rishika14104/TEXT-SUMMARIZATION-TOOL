import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import WordNetLemmatizer
import string
import numpy as np

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    # Lowercase the text and remove punctuation
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

def lemmatize_words(words):
    return [lemmatizer.lemmatize(word) for word in words]

def summarize_text(text, summary_length='short'):
    cleaned_text = clean_text(text)
    sentences = sent_tokenize(text)

    # Lemmatize words and tokenize
    words = word_tokenize(cleaned_text)
    lemmatized_words = lemmatize_words(words)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in lemmatized_words if word.isalpha() and word not in stop_words]

    # TF-IDF vectorizer
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)
    feature_names = vectorizer.get_feature_names_out()

    # Sentence scoring using TF-IDF
    sentence_scores = {}
    for i, sentence in enumerate(sentences):
        score = 0
        sentence_tokens = word_tokenize(clean_text(sentence))
        lemmatized_sentence = lemmatize_words(sentence_tokens)

        # Score sentence based on word importance
        for word in lemmatized_sentence:
            if word in feature_names:
                word_index = feature_names.tolist().index(word)
                score += tfidf_matrix[i, word_index]

        # Normalize score by sentence length
        sentence_scores[sentence] = score / len(sentence_tokens) if len(sentence_tokens) > 0 else 0

    # Determine the number of sentences to include in the summary
    if summary_length == 'short':
        num_sentences = 1
    elif summary_length == 'medium':
        num_sentences = 3
    elif summary_length == 'long':
        num_sentences = 5
    else:
        raise ValueError("Invalid summary length. Choose 'short', 'medium', or 'long'.")

    # Sort sentences by score
    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)

    # For short summaries, select the most representative sentence
    if summary_length == 'short':
        # Select the highest scoring sentence
        summary = sorted_sentences[0]
    else:
        # For medium and long summaries, select the top sentences
        summary = ' '.join(sorted_sentences[:num_sentences])

    return summary

if __name__ == "__main__":
    text = input("Enter the text to summarize:\n")
    summary_length = input("Specify the length of the summary (short, medium, long):\n").lower()
    summary = summarize_text(text, summary_length)
    print("\nSummary:\n", summary)
